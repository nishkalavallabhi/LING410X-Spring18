stylo()
library(stylo)
stylo()
install.packages("updateR")
lista <- list(1,2,3,4)
listb <- list(1, "a", "b", TRUE)
vectora <- c(1,2,3,4)
vectora
lista
listb
vectorb <- c(1,"aa2",3,4)
vectorb
?readLines
html <- readLines("http://www.gutenberg.org/files/2446/2446-h/2446-h.htm")
nchar(html)
sum(nchar(html))
text <- readlines("http://www.gutenberg.org/ebooks/2446.txt.utf-8")
text <- readLines("http://www.gutenberg.org/ebooks/2446.txt.utf-8")
nchar(text)
sum(nchar(text))
guardian_key <- "8b40468e-139b-4d90-94ac-1f74d22fd291"
library(GuardianR)
results <- get_guardian("George+Washington", guardian_key)
results <- get_guardian("George+Washington", guardian_key, to.date = "01/01/2011", from.date = "01/01/2015")
results <- get_guardian("George+Washington", apu.key = guardian_key, to.date = "01/01/2011", from.date = "01/01/2015")
results <- get_guardian("George+Washington", api.key = guardian_key, to.date = "01/01/2011", from.date = "01/01/2015")
results <- get_guardian("George+Bush", api.key = guardian_key, to.date = "01/01/2011", from.date = "01/01/2015")
results <- get_guardian("George+Town", api.key = guardian_key, to.date = "01/01/2011", from.date = "01/01/2015")
results <- get_guardian("George", api.key = guardian_key, to.date = "01/01/2011", from.date = "01/01/2015")
results <- get_guardian("George+Town", api.key = guardian_key, to.date = "01-01-2011", from.date = "01-01-2015")
results <- get_guardian("George+Town", api.key = guardian_key, to.date = "2011-01-01", from.date = "2015-01-01")
results <- get_guardian("George+Washington", api.key = guardian_key, to.date = "2011-01-01", from.date = "2015-01-01")
results <- get_guardian("Donald+Trump", api.key = guardian_key, to.date = "2011-01-01", from.date = "2015-01-01")
results <- get_guardian("united+states", section="world", from.date="2014-09-16", to.date="2014-09-16", api.key="8b40468e-139b-4d90-94ac-1f74d22fd291")
results <- get_guardian("Donald+Trump", section=“world”, api.key = guardian_key, to.date = "2011-01-01", from.date = "2015-01-01")
results <- get_guardian("Donald+Trump", section= "world", api.key = guardian_key, to.date = "2011-01-01", from.date = "2015-01-01")
results <- get_guardian("donald+trump", section= "world", api.key = guardian_key, to.date = "2011-01-01", from.date = "2015-01-01")
results <- get_guardian("donald+trump", section= "world", api.key = guardian_key, to.date = "2015-01-01", from.date = "2011-01-01")
results <- get_guardian("donald+trump", section= "world", api.key = guardian_key, to.date = "2015-01-01", from.date = "2015-02-01")
results <- get_guardian("donald+trump", section= "world", api.key = guardian_key, to.date = "2015-02-01", from.date = "2015-01-01")
typeof(results)
results
names(results)
results$sectionName
results$headline
length(results)
results[[1]]$headline
results[[1]]
results[[2]]
results[[3]]
results[[4]]
results["headline"]
results <- get_guardian("donald+trump", section= "world", api.key = guardian_key, to.date = "2015-02-01", from.date = "2015-01-01")
names(results)
data.frame(results[3],results[10])
data.frame(results["sectionName"],results["headline"])
my_df <- data.frame(results["sectionName"],results["headline"])
write.csv(my_df, file = "results.csv")
getwd()
source('~/Dropbox/ClassroomSlides-BothCourses/LING410X/Resources/14Feb2017/KWIC.R', echo=TRUE)
setwd("~/Dropbox/ClassroomSlides-BothCourses/LING410X/Resources/14Feb2017")
source('~/Dropbox/ClassroomSlides-BothCourses/LING410X/Resources/14Feb2017/KWIC.R', echo=TRUE)
source('~/Dropbox/ClassroomSlides-BothCourses/LING410X/Resources/14Feb2017/KWIC.R', echo=TRUE)
setwd("~/Dropbox/ClassroomSlides-BothCourses/LING410X/Resources/14Feb2017")
source('~/Dropbox/ClassroomSlides-BothCourses/LING410X/Resources/14Feb2017/KWIC.R', echo=TRUE)
source('~/Dropbox/ClassroomSlides-BothCourses/LING410X/Resources/14Feb2017/KWIC.R', echo=TRUE)
source('~/Dropbox/ClassroomSlides-BothCourses/LING410X/Resources/14Feb2017/KWIC.R', echo=TRUE)
Gender = c("Female", "Male", "Male", "Female", "Female", "Female")
NewOrFollowUp = c("New", "New", "New", "Follow-up", "New", "Follow-up")
Points = c(1,12,32,4,55,21)
example_xtab <- data.frame(Gender, NewOrFollowUp, Points)
example_xtab
xtabs(Points ~ Gender, data=example_xtab)
xtabs(Points ~ Gender + NewOrFollowUp, data=example_xtab)
library(tm)
?removeSparseTerms
makeFlexTextChunks <- function(doc.object, chunk.size=1000, percentage=TRUE){
paras <- getNodeSet(doc.object,
"/d:TEI/d:text/d:body//d:p",
c(d = "http://www.tei-c.org/ns/1.0"))
words <- paste(sapply(paras,xmlValue), collapse=" ")
words.lower <- tolower(words)
words.lower <- gsub("[^[:alnum:][:space:]']", " ", words.lower)
words.l <- strsplit(words.lower, "\\s+")
word.v <- unlist(words.l)
x <- seq_along(word.v)
if(percentage){
max.length <- length(word.v)/chunk.size
chunks.l <- split(word.v, ceiling(x/max.length))
} else {
chunks.l <- split(word.v, ceiling(x/chunk.size))
#deal with small chunks at the end
if(length(chunks.l[[length(chunks.l)]]) <=
length(chunks.l[[length(chunks.l)]])/2){
chunks.l[[length(chunks.l)-1]] <-
c(chunks.l[[length(chunks.l)-1]],
chunks.l[[length(chunks.l)]])
chunks.l[[length(chunks.l)]] <- NULL
}
}
chunks.l <- lapply(chunks.l, paste, collapse=" ")
chunks.df <- do.call(rbind, chunks.l)
return(chunks.df)
wd <- "~/Dropbox/ClassroomSlides-BothCourses/LING410X/Resources/TextAnalysisWithR-Textbook/Suppl.Materials/"
inputDir <- paste(wd,"data/XMLAuthorCorpus",sep="")
stopwords_path <- paste(wd, "data/stoplist.csv",sep="")
files.v <- dir(path=inputDir , pattern=".*xml")
chunk.size <- 1000
topic.m <- NULL
for(i in 1:length(files.v)){
doc.object <- xmlTreeParse(file.path(inputDir, files.v[i]),
useInternalNodes=TRUE)
chunk.m <- makeFlexTextChunks(doc.object, chunk.size,
percentage=FALSE)
textname <- gsub("\\..*","", files.v[i])
segments.m <- cbind(paste(textname,
segment=1:nrow(chunk.m), sep="_"), chunk.m)
topic.m <- rbind(topic.m, segments.m)
}
documents <- as.data.frame(topic.m, stringsAsFactors=F)
colnames(documents) <- c("id", "text")
library(mallet)
mallet.instances <- mallet.import(documents$id,documents$text,stopwords_path,FALSE,
token.regexp="[\\p{L}']+")
topic.model <- MalletLDA(num.topics=43)
topic.model$loadDocuments(mallet.instances)
#The following line is optional.
#Mallet takes default parameters if you don't specify anything here.
topic.model$setAlphaOptimization(40, 80)
#This starts the training process
topic.model$train(400)
)
makeFlexTextChunks <- function(doc.object, chunk.size=1000, percentage=TRUE){
paras <- getNodeSet(doc.object,
"/d:TEI/d:text/d:body//d:p",
c(d = "http://www.tei-c.org/ns/1.0"))
words <- paste(sapply(paras,xmlValue), collapse=" ")
words.lower <- tolower(words)
words.lower <- gsub("[^[:alnum:][:space:]']", " ", words.lower)
words.l <- strsplit(words.lower, "\\s+")
word.v <- unlist(words.l)
x <- seq_along(word.v)
if(percentage){
max.length <- length(word.v)/chunk.size
chunks.l <- split(word.v, ceiling(x/max.length))
} else {
chunks.l <- split(word.v, ceiling(x/chunk.size))
#deal with small chunks at the end
if(length(chunks.l[[length(chunks.l)]]) <=
length(chunks.l[[length(chunks.l)]])/2){
chunks.l[[length(chunks.l)-1]] <-
c(chunks.l[[length(chunks.l)-1]],
chunks.l[[length(chunks.l)]])
chunks.l[[length(chunks.l)]] <- NULL
}
}
chunks.l <- lapply(chunks.l, paste, collapse=" ")
chunks.df <- do.call(rbind, chunks.l)
return(chunks.df)
}
library(XML)
wd <- "~/Dropbox/ClassroomSlides-BothCourses/LING410X/Resources/TextAnalysisWithR-Textbook/Suppl.Materials/"
inputDir <- paste(wd,"data/XMLAuthorCorpus",sep="")
stopwords_path <- paste(wd, "data/stoplist.csv",sep="")
files.v <- dir(path=inputDir , pattern=".*xml")
chunk.size <- 1000
topic.m <- NULL
for(i in 1:length(files.v)){
doc.object <- xmlTreeParse(file.path(inputDir, files.v[i]),
useInternalNodes=TRUE)
chunk.m <- makeFlexTextChunks(doc.object, chunk.size,
percentage=FALSE)
textname <- gsub("\\..*","", files.v[i])
segments.m <- cbind(paste(textname,
segment=1:nrow(chunk.m), sep="_"), chunk.m)
topic.m <- rbind(topic.m, segments.m)
}
documents <- as.data.frame(topic.m, stringsAsFactors=F)
colnames(documents) <- c("id", "text")
library(mallet)
mallet.instances <- mallet.import(documents$id,documents$text,stopwords_path,FALSE,
token.regexp="[\\p{L}']+")
topic.model <- MalletLDA(num.topics=43)
topic.model$loadDocuments(mallet.instances)
#The following line is optional.
#Mallet takes default parameters if you don't specify anything here.
topic.model$setAlphaOptimization(40, 80)
#This starts the training process
topic.model$train(400)
library(mallet)
mallet.instances <- mallet.import(documents$id,documents$text,stopwords_path,FALSE,
token.regexp="[\\p{L}']+")
topic.model <- MalletLDA(num.topics=10)
topic.model$loadDocuments(mallet.instances)
#The following line is optional.
#Mallet takes default parameters if you don't specify anything here.
topic.model$setAlphaOptimization(40, 80)
#This starts the training process
topic.model$train(400)
?MalletLDA
doc.topics <- mallet.doc.topics(topic.model, smoothed=T, normalized=T)
doc.topics
doc.topics
sum(doc.topics[1,])
sum(doc.topics[2,])
doc.topics <- mallet.doc.topics(topic.model, smoothed=T, normalized=T)
topic.words <- mallet.topic.words(topic.model, smoothed=T, normalized=T)
doc.topics[10,]
doc.topics
topic.words
doc.topics <- mallet.doc.topics(topic.model, smoothed=T, normalized=T)
topic.words <- mallet.topic.words(topic.model, smoothed=T, normalized=T)
topic.words <- mallet.topic.words(topic.model, smoothed=T, normalized=T)
topic.words
mallet.top.words(topic.model, topic.words[7,])
mallet.top.words(topic.model, topic.words[7,], normalized=TRUE)
topic.words[1,]
colnames(topic.words)
dim(topic.words)
library(stylo)
stylo()
install.packages(h20)
install.packages("h20")
update.packages()
install.packages("h20")
install.packages("h20")
library(xml)
install.packages("XML")
library(xml)
install.packages("xml")
library(XML)
library(XML)
books <- xmlTreeParse("Books.xml", useInternalNodes = TRUE)
library(XML)
url <- "http://radar.oreilly.com/2011/09/building-data-science-teams.html"
doc <- htmlParse(url, useInternal = TRUE)
links <-xpathSApply(doc, "//a/@href")
length(doc)
library(swirl)
install.packages("swirl")
library(swirl)
swirl()
getwd()
ls()
x <- 9
ls()
print("A")
print("B")
cat("A")
cat("B")
setwd("~/Dropbox/ClassroomSlides-BothCourses/LING410X/Week3Materials/code")
source('~/Dropbox/ClassroomSlides-BothCourses/LING410X/Week3Materials/code/Directory.R', echo=TRUE)
setwd("~/Dropbox/ClassroomSlides-BothCourses/LING410X/Week3Materials/code/mydata")
source('~/Dropbox/ClassroomSlides-BothCourses/LING410X/Week3Materials/code/Directory.R', echo=TRUE)
source('~/Dropbox/ClassroomSlides-BothCourses/LING410X/Week3Materials/code/Directory.R', echo=TRUE)
file.names <- dir(getwd(), pattern ="\\.txt")
file.names
data <- vector()
for (f in file.names) {
tempData <- scan(f, what="character", sep="\n")
tempDataString <- tolower(paste(tempData, collapse = " "))
data <- c(data,tempDataString)
}
length(data)
length(data[1])
length(data[1][1])
data[1]
nchar(data[1])
nchar(data[2])
nchar(data[3])
nchar(data)
file.names <- dir("/home/bangaru/Desktop")
for (f in file.names) {
print(f)
}
file.names <- dir("/Users/sowmya/Desktop")
for (f in file.names) {
print(f)
}
file.names <- dir(getwd(), pattern ="\\.txt")
file.names
data <- vector()
for (f in file.names) {
tempData <- scan(f, what="character", sep="\n")
tempDataString <- tolower(paste(tempData, collapse = " "))
data <- c(data,tempDataString)
}
for(item in data)
{
words <- strsplit(item, "\\W+")
sorted_freqs <- sort(table(words), decreasing = TRUE)
print(sorted_freqs[1:10]
}
for(item in data)
{
words <- strsplit(item, "\\W+")
sorted_freqs <- sort(table(words), decreasing = TRUE)
print(sorted_freqs[1:10])
}
names(res1$data)
res2 <- as_search(q="artificial intelligence", begin_date = "20180101", end_date = "20180120")
install.packages("rtimes")
library(rtimes)
Sys.setenv(NYTIMES_AS_KEY = "c67c9de54f894135ac568dda4f7679ee")
res1 <- as_search(q="artificial intelligence", begin_date = "20081001", end_date = "20081201")
names(res1$data)
df <- data.frame(res1$data$snippet,res1$data$pub_date)
write.csv(df,"temp.csv")
setwd("~/Dropbox/ClassroomSlides-BothCourses/LING410X/Week6Mats/13-15Feb2018")
library(ngram)
#The usual stuff
dollshouse_text <- scan("DollsHouse-Eng.txt", what = "character", sep = "\n")
dollshouse_string <- paste(dollshouse_text, collapse = " ")
dollshouse_string <- tolower(dollshouse_string)
dollshouse_string <- gsub("[[:punct:]]", " ", dollshouse_string)
dollshouse_string <- gsub(" +", " ", dollshouse_string)
#ngram() function expects a "string" not a vector of words.
unigrams <- ngram(dollshouse_string,n=1)
bigrams <- ngram(dollshouse_string,n=2)
trigrams <- ngram(dollshouse_string,n=3)
#Just get the vector of all ngrams without all additional information
bigrams_vector <- get.ngrams(bigrams)
trigrams_vector <- get.ngrams(trigrams)
install.packages("ngram")
library(ngram)
#The usual stuff
dollshouse_text <- scan("DollsHouse-Eng.txt", what = "character", sep = "\n")
dollshouse_string <- paste(dollshouse_text, collapse = " ")
dollshouse_string <- tolower(dollshouse_string)
dollshouse_string <- gsub("[[:punct:]]", " ", dollshouse_string)
dollshouse_string <- gsub(" +", " ", dollshouse_string)
#ngram() function expects a "string" not a vector of words.
unigrams <- ngram(dollshouse_string,n=1)
bigrams <- ngram(dollshouse_string,n=2)
trigrams <- ngram(dollshouse_string,n=3)
#Just get the vector of all ngrams without all additional information
bigrams_vector <- get.ngrams(bigrams)
trigrams_vector <- get.ngrams(trigrams)
top10unigrams <- head(get.phrasetable(unigrams),n = 10)
top10bigrams <- head(get.phrasetable(bigrams),n = 10)
top10trigrams <- head(get.phrasetable(trigrams),n = 10)
typeof(top10bigrams)
names(top10bigrams)
top10bigrams$freq
plot(top10trigrams$ngrams,top10trigrams$freq)
plot(top10trigrams$freq, type="b")
top10trigrams[[2]]
top10trigrams$freq
plot(top10trigrams$freq)
plot(top10trigrams[[2]])
plot(top10trigrams$freq, type="b", xlab = top10trigrams$ngrams)
plot(top10trigrams$freq, type="b", x = top10trigrams$ngrams)
install.packages("reticulate")
library(mallet)
setwd("~/Dropbox/ClassroomSlides-BothCourses/LING410X/21Mar2018/data")
setwd("~/Dropbox/ClassroomSlides-BothCourses/LING410X/21Mar2018/data/reviews")
stoplistfile = "/home/bangaru/Dropbox/ClassroomSlides-BothCourses/LING410X/21Mar2018/stoplist.csv"
?mallet.read.dir
mallet.instances <- mallet.read.dir("~/Dropbox/ClassroomSlides-BothCourses/LING410X/21Mar2018/data/reviews")
documents <- mallet.read.dir("~/Dropbox/ClassroomSlides-BothCourses/LING410X/21Mar2018/data/reviews")
mallet.instances <- mallet.import(documents$id,documents$text,stoplistfile,FALSE,token.regexp="[\\p{L}']+")
stoplistfile = "/Users/sowmya/Dropbox/ClassroomSlides-BothCourses/LING410X/21Mar2018/stoplist.csv"
mallet.instances <- mallet.import(documents$id,documents$text,stoplistfile,FALSE,token.regexp="[\\p{L}']+")
topic.model <- MalletLDA(num.topics=5)
topic.model$loadDocuments(mallet.instances)
vocabulary <- topic.model$getVocabulary()
length(vocabulary)
head(vocabulary)
vocabulary[1:50]
word.freqs <- mallet.word.freqs(topic.model)
topic.model$setAlphaOptimization(40, 80)
topic.model$train(100)
topic.words.m <- mallet.topic.words(topic.model,smoothed=TRUE,normalized=TRUE)
dim(topic.words.m)
library(wordcloud)
topic.top.words <- mallet.top.words(topic.model,topic.words.m[1,], 100)
topic.top.words
wordcloud(topic.top.words$words,topic.top.words$weights,c(4,.8), rot.per=0, random.order=F)
topic.top.words <- mallet.top.words(topic.model,topic.words.m[2,], 100)
wordcloud(topic.top.words$words,topic.top.words$weights,c(4,.8), rot.per=0, random.order=F)
documents <- mallet.read.dir("~/Dropbox/ClassroomSlides-BothCourses/LING410X/21Mar2018/data/reviews")
stoplistfile = "/Users/sowmya/Dropbox/ClassroomSlides-BothCourses/LING410X/21Mar2018/stoplist.csv"
mallet.instances <- mallet.import(documents$id,documents$text,stoplistfile,FALSE,token.regexp="[\\p{L}']+")
topic.model <- MalletLDA(num.topics=5)
topic.model$loadDocuments(mallet.instances)
topic.model$train(100)
topic.model
topic.model$model
topic.model$`getDocumentNames()`
topic.model$getDocumentNames()
topic.model$getTopicWords()
?mallet.topic.hclust
mallet.topic.hclust(topic.model, topic.words.m, balance)
doc.topics.m <- mallet.doc.topics(topic.model)
mallet.topic.hclust(doc.topics.m, topic.words.m, 0.5)
topic.labels <- mallet.topic.hclust(doc.topics.m, topic.words.m, 0.5)
plot(mallet.topic.hclust(doc.topics, topic.words, 0.3), labels=topic.labels)
topic.words.m
topic.words.m[1:3,1:3]
vocabulary <- topic.model$getVocabulary()
vocabulary
vocabulary[1:50]
head(vocabulary)
length(vocabulary)
word.freqs <- mallet.word.freqs(topic.model)
word.freqs
topic.top.words <- mallet.top.words(topic.model,topic.words.m[1,], 100)
topic.top.words
topic.top.words <- mallet.top.words(topic.model,topic.words.m[1,], 5)
topic.top.words
topic.top.words <- mallet.top.words(topic.model,topic.words.m[2,], 5)
topic.top.words
topic.top.words <- mallet.top.words(topic.model,topic.words.m[3,], 5)
topic.top.words
topic.top.words <- mallet.top.words(topic.model,topic.words.m[4,], 5)
topic.top.words
topic.top.words <- mallet.top.words(topic.model,topic.words.m[5,], 5)
topic.top.words
topic.top.words.1 <- mallet.top.words(topic.model,topic.words.m[1,], 5)
topic.top.words.2 <- mallet.top.words(topic.model,topic.words.m[2,], 15)
topic.top.words.3 <- mallet.top.words(topic.model,topic.words.m[3,], 100)
wordcloud(topic.top.words.3$words,topic.top.words.3$weights,c(4,.8), rot.per=0, random.order=F)
wordcloud(topic.top.words.2$words,topic.top.words.2$weights,c(4,.8), rot.per=0, random.order=F)
documents <- mallet.read.dir("/Users/sowmya/Downloads/UsefulDatasets/comcore/data")
stoplistfile = "/Users/sowmya/Dropbox/ClassroomSlides-BothCourses/LING410X/21Mar2018/stoplist.csv"
mallet.instances <- mallet.import(documents$id,documents$text,stoplistfile,FALSE,token.regexp="[\\p{L}']+")
topic.model <- MalletLDA(num.topics=15)
topic.model$loadDocuments(mallet.instances)
topic.model$train(100)
topic.words.m <- mallet.topic.words(topic.model,smoothed=TRUE,normalized=TRUE)
doc.topics.m <- mallet.doc.topics(topic.model)
word.freqs <- mallet.word.freqs(topic.model)
topic.top.words.3 <- mallet.top.words(topic.model,topic.words.m[3,], 100)
wordcloud(topic.top.words.3$words,topic.top.words.3$weights,c(4,.8), rot.per=0, random.order=F)
topic.top.words.2 <- mallet.top.words(topic.model,topic.words.m[2,], 100)
wordcloud(topic.top.words.2$words,topic.top.words.2$weights,c(4,.8), rot.per=0, random.order=F)
topic.labels <- mallet.topic.hclust(doc.topics.m, topic.words.m, 0.5)
plot(mallet.topic.hclust(doc.topics, topic.words, 0.3), labels=topic.labels)
plot(topic.labels, labels=topic.labels)
?mallet.topic.hclust
topic.labels <- mallet.topic.hclust(topic.model, topic.words.m, 0.5)
topic.labels <- mallet.topic.labels(topic.model, topic.words.m, 0.5)
topic.labels
plot(mallet.topic.hclust(doc.topics.m, topic.words.m, 0.3), labels=topic.labels.m)
plot(mallet.topic.hclust(doc.topics.m, topic.words.m, 0.3), labels=topic.labels)
?mallet
?mallet.subset.topic.words
?mallet
