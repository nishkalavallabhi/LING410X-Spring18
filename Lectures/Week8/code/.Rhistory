chapter.string <- tolower(paste(chapter.lines, collapse = " "))
chapter.string <- gsub(" +", " ", gsub("[[:punct:]]", " ", chapter.string))
chapter.words <- unlist(strsplit(chapter.string, "\\W"))
chapter.freqs <- table(chapter.words)
chapters.raw[[title]] <- chapter.freqs
}
chapters.raw[[1]]
sum(chapters.raw[[1]])
length(chapers[[1]])
length(chapters.raw[[1]])
sum_of_all_word_freqs_chapter1 <- sum(chapters_raw[[1]])
num_words_in_chapter1 <- length(chapters_raw[[1]])
mean_word_freq_chapter1 <- sum_of_all_word_freqs_chapter1/num_words_in_chapter1
sum_of_all_word_freqs_chapter1 <- sum(chapters.raw[[1]])
num_words_in_chapter1 <- length(chapters.raw[[1]])
mean_word_freq_chapter1 <- sum_of_all_word_freqs_chapter1/num_words_in_chapter1
mean_word_freq_chapter1
mean(chapters.raw[[1]])
lapply(chapters.raw, "[[")
lapply(chapters.raw, "[")
?sapply
lapply(chapters.raw, mean)
lapply(chapters.raw, mean)
length(chapters.raw)
means = c()
for(i in 1:length(chapters.raw))
{
means[i] <- sum(chapters.raw[[i]])/length(chapters.raw)
}
means
means = c()
for(i in 1:length(chapters.raw))
{
means[i] <- sum(chapters.raw[[i]])/length(chapters.raw[[i]])
}
means
matrix(lapply(chapters.raw,mean))
vector(lapply(chapters.raw,mean))
?rbind
rbind(lapply(chapters.raw, mean))
?lapply
sapply(chapters.raw, mean)
trial <- sapply(chapters.raw, mean)
class(trial)
typeof(trial)
trial <- sapply(chapters.raw, mean, simplify="vector")
trial
trial[[1]]
trial[[2]]
plot(trial)
trial <- sapply(chapters.raw, mean)
trial <- lapply(chapters.raw, mean)
plot(trial)
trial <- sapply(chapters.raw, mean)
trial <- lapply(chapters.raw, mean)
trial
typeof(trial)
unname(trial)
unlist(unname(trial))
trial <- lapply(chapters.raw, mean)
unlist(trial)
trial <- lapply(chapters.raw, mean)
unlist(trial)
unname(unlist(trial))
means <- lapply(chapters.raw, mean)
means <- unlist(means)
means <- unname(means)
means
par(mfrow=c(1,2))
plot(means, type="h")
plot(scale(means), type = "h")
means <- lapply(chapters.raw, mean)
sort(means)
means <- unlist(means)
sort(means)
sort(means, decreasing=TRUE)
hapax <- sapply(chapters.raw, function(x) sum(x == 1))
hapax
unname(hapax)
unlist(hapax)
unname(hapax)
lapply(chapters.raw,length)
lapply(chapters.raw,sum)
hapax <- sapply(chapters.raw, function(x) sum(x == 1))
hapax <- unname(hapax)
lengths <- lapply(chapters.raw, sum)
new <- hapax/lengths
lengths
hapax <- sapply(chapters.raw, function(x) sum(x == 1))
hapax <- unname(hapax)
lengths <- lapply(chapters.raw, sum)
new <- hapax/lengths
hapax <- sapply(chapters.raw, function(x) sum(x == 1))
hapax <- unname(hapax)
lengths <- unname(sapply(chapters.raw, sum))
new <- hapax/lengths
new
?par
hapax <- sapply(chapters.raw, function(x) sum(x == 1:5))
hapax
moby <- scan("mobydick.txt", what = "character", sep = "\n")
moby.start <- which (moby ==  "CHAPTER 1. Loomings.")
moby.end <- which (moby == "orphan.")
moby.actual <- moby[moby.start:moby.end]
moby <- scan("mobydick.txt", what = "character", sep = "\n")
moby.start <- which (moby ==  "CHAPTER 1. Loomings.")
moby.end <- which (moby == "orphan.")
moby.actual <- moby[moby.start:moby.end]
moby.actual <- tolower(paste(moby.actual, collapse = " "))
moby.actual <- gsub(" +", " ", gsub("[[:punct:]]", " ", moby.actual)
moby.words <- unlist(strsplit(moby.actual, "\\W"))
moby.freqs <- table(moby.words)
moby <- scan("mobydick.txt", what = "character", sep = "\n")
moby.start <- which (moby ==  "CHAPTER 1. Loomings.")
moby.end <- which (moby == "orphan.")
moby.actual <- moby[moby.start:moby.end]
moby.actual <- tolower(paste(moby.actual, collapse = " "))
moby.actual <- gsub(" +", " ", gsub("[[:punct:]]", " ", moby.actual)
)
moby.words <- unlist(strsplit(moby.actual, "\\W"))
moby.freqs <- table(moby.words)
moby.freqs
max_freq <- sort(moby.freqs, decreasing = TRUE)
max_freq <- sort(moby.freqs, decreasing = TRUE)[1]
max_freq
max_freq <- sort(moby.freqs, decreasing = TRUE)[[1]]
max_freq
what_i_want <- c()
for(i in min_freq:max_freq)
{
what_i_want[i] <- length(sapply(moby.freqs, function(x) x==i))
}
what_i_want
min_freq <- 1
what_i_want <- c()
for(i in min_freq:max_freq)
{
what_i_want[i] <- length(sapply(moby.freqs, function(x) x==i))
}
min_freq <- 1
what_i_want <- c()
for(i in min_freq:100)
{
what_i_want[i] <- length(sapply(moby.freqs, function(x) x==i))
}
what_i_want
what_i_want <- c()
for(i in min_freq:100)
{
what_i_want[i] <- length(sapply(moby.freqs, function(x) sum(x==i)))
}
what_i_want
moby.freqs
max_freq
sapply(moby.freqs, function(x) sum(x)==1)
max_freq
sapply(moby.freqs, function(x) sum(x)==14715)
length(sapply(moby.freqs, function(x) sum(x)==14715))
sapply(moby.freqs, function(x) sum(x)==14715)
sapply(moby.freqs, function(x) sum(x)==14715)["TRUE"]
length(which(sapply(moby.freqs, function(x) sum(x)==14715))
)
length(which(sapply(moby.freqs, function(x) sum(x)==1))
)
length(which(sapply(moby.freqs, function(x) sum(x)==1)))
max_freq
length(which(sapply(moby.freqs, function(x) sum(x)==14175)))
for(i in min_freq:100)
{
what_i_want[i] <- length(which(sapply(moby.freqs, function(x) sum(x)==14175)))
}
plot(what_i_want)
for(i in min_freq:100)
{
what_i_want[i] <- length(which(sapply(moby.freqs, function(x) sum(x)==i)))
}
plot(what_i_want)
plot(unname(sort(moby.freqs, decreasing = TRUE)))
plot(moby.freqs)
freqslist <- unname(sort(moby.freqs, decreasing = TRUE))
freqslist
plot(freqslist, type="b")
plot(freqslist)
plot(unlist(freqslist))
typeof(freqslist)
freqslist <- sort(moby.freqs, decreasing = TRUE)
freqslist <- unlist(sort(moby.freqs, decreasing = TRUE))
freqslist
length(moby.freqs)
length(freqslist)
vec <- unname(freqslist)
typeof(vec)
class(vec)
plot(vec)
class(freqslist)
library(mallet)
install.packages("rJava")
R
quit()
library("rJava")
library("mallet")
quit()
library(stylo)
mycsvfile[1:3]
file_path = "/home/bangaru/Dropbox/ClassroomSlides-BothCourses/LING410X/Resources/20Apr2017/twitter-sentiment.csv"
mycsvfile = read.csv(file_path, head=TRUE)
colnames(mycsvfile)
#This gives you column names in the csv file. You can choose the column name you want by using $ sign.
#mycsvfile$ItemID #Gives you the entire column.
#mycsvfile[,1] #Also does the same thing because that is the first column.
#mycsvfile[1,] #Gives you the first ROW of this file after the header.
mycsvfile[1:3]
mycsvfile[,1:3]
library(GuardianR)
results <- get_guardian("united+states", section="world", from.date="2014-09-16", to.date="2014-09-16", api.key="8b40468e-139b-4d90-94ac-1f74d22fd291")
names(results)
nrow(results)
ncol(results)
resutls1]
resutls[1]
resutls["1"]
results[1]
results[2]
library(ngram)
text = "Trying this out right now to see see it see to see"
unigrams <- ngram(text, n=2)
unigrams
head(get.phrasetable(unigrams),n = 6)
install.packages("swirl")
library(swirl)
swirl()
1:20
pi:10
15:1
?:
?`:`
seq(1,20)
seq(0,10,by=0.5)
seq(5,10,length=30)
my_seq <- seq(5,10,length=30)
length(my_seq)
1:length(my_seq)
seq(along.with = my_seq)
seq_along(my_seq)
rep(0,times=40)
rep(c(0,1,2),times=10)
rep(c(0,1,2), each=10)
3+4
c = "Sowmya"
c
install.packages("swirl")
library(swirl)
swirl()
5+7
?abs
?paste
5+7
x ,-5 + 7
x <-5 + 7
0
0
x
quit()
list4 <- list(first="Sowmya", course=410, office=331, address="Ross")
list4[1]
list4[[1]]
library(rtimes)
Sys.setenv(NYTIMES_AS_KEY = "c67c9de54f894135ac568dda4f7679ee")
res1 <- as_search(q="artificial intelligence", begin_date = "20081001", end_date = "20081201")
res1$data$snippet
res1
res1$data
res1$data$Snippet
res1$data["Snippet"]
names(res1$data)
res1
names(res1)
names(res1$data)
res1$data
res1$data[[1]]
install.packages("rtimes")
Sys.setenv(NYTIMES_AS_KEY = "c67c9de54f894135ac568dda4f7679ee")
library(rtimes)
res1 <- as_search(q="artificial intelligence", begin_date = "20081001", end_date = "20081201")
names(res1)
res1$data
namse(res1$data)
names(res1$data)
list4 <- list(first="Sowmya", course=410, office=331, address="Ross")
vector4 <- c(first="Sowmya", course=410, office=331, address="Ross")
names(list4)
names(vector4)
vector4["course"]
list4
list4[1]
list4[[1]]
is.list(vector4)
is.vector(vector4)
library(rtimes)
Sys.setenv(NYTIMES_AS_KEY = "c67c9de54f894135ac568dda4f7679ee")
res1 <- as_search(q="artificial intelligence", begin_date = "20081001", end_date = "20081201")
res1
names(res1)
res1$data
names(res1$data)
res1$data$web_url
res1$data$snippet
snippets_2008 <-  res1$data$snippet
for (snippet in snippets_2008) {
print(tolower(snippet))
}
df <- data.frame(res1$data$snippet,res1$data$pub_date)
write.csv(df,"temp.csv")
getwd()
library(tm)
?colSums
?sample
sample.int(100, size=5)
as.matrix(sample.int(1,100),5,5)
as.matrix(sample.int(100),5,5)
m <- as.matrix(sample.int(100),5,5)
m
m <- as.matrix(5,5)
m
m <- matrix(5,5)
m
m <- matrix(sample.int(100),5,5)
m
colSums(m)
?colSums
m <- matrix(sample.int(100),5,5)
rowSums(m)
colSums(m)
rowMeans(m)
colMeans(m)
m
37+95+26+16+1
sample.int(19)
sample.int(100)
m <- matrix(sample.int(100),5,5)
m
m <- matrix(sample.int(100),5,5)
m
rowSums()
rowSums(m)
100+29+3+86+65
colSums(m)
m
rowMeans(m)
colMeans(m)
colSums(m)
library(tm)
library(e1071)
library(SnowballC) #for stemming.
setwd("~/Dropbox/ClassroomSlides-BothCourses/LING410X/Week8Mats/code")
cleanCorpus <- function(folderpath)
{
corpus <- VCorpus(DirSource(folderpath))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus,stemDocument)
return(corpus)
#Note: getTransformations() gives full list of possible transformations.
#stopwords("English") will give you all stopwords.
}
#Create training corpus
corpus_train_pos <- cleanCorpus("data/train/pos") #has 234 examples of positive reviews
corpus_train_neg <- cleanCorpus("data/train/neg") #has 234 examples of negative reviews
corpus_train <- c(corpus_train_pos,corpus_train_neg) #has 468 examples in total
training_size <- length(corpus_train_pos) + length(corpus_train_neg)
#Create testing corpus
corpus_test_pos <- cleanCorpus("data/test/pos") #82 positive docs for testing
corpus_test_neg <- cleanCorpus("data/test/neg") #67 negative docs for testing
corpus_test <- c(corpus_test_pos,corpus_test_neg) #has 149 examples in total
#Combine them, to create a document term matrix for the whole data first
fullcorpus <- c(corpus_train,corpus_test) #has 617 texts.
#training: 234 positive, 234 negative, followed by testing:82 positive, 67 negative
dtm_together <- DocumentTermMatrix(fullcorpus)
freq <- colSums(as.matrix(dtm_together))
ncol(dtm_together)
freq
freq <- sort(colSums(as.matrix(dtm_together)), decreasing=TRUE)
freq[1:10]
freq[10:20]
findFreqTerms(dtm_together, lowfreq=100, highfreq=500)
findFreqTerms(dtm_together, lowfreq=500, highfreq=1500)
findFreqTerms(dtm_together, lowfreq=1200, highfreq=1500)
findAssocs(dtm_together, "good", corlimit = 0.3)
findAssocs(dtm_together, "good", corlimit = 0.4)
findAssocs(dtm_together, "bad", corlimit = 0.4)
findAssocs(dtm_together, "bad", corlimit = 0.3)
findAssocs(dtm_together, "bad", corlimit = 0.2)
findAssocs(dtm_together, "like", corlimit = 0.3)
findAssocs(dtm_together, "like", corlimit = 0.2)
findAssocs(dtm_together, "film", corlimit = 0.3)
findAssocs(dtm_together, "actor", corlimit = 0.3)
findAssocs(dtm_together, "act", corlimit = 0.3)
findAssocs(dtm_together, "action", corlimit = 0.3)
findAssocs(dtm_together, "horror", corlimit = 0.3)
findAssocs(dtm_together, "horror", corlimit = 0.4)
findAssocs(dtm_together, "horror", corlimit = 0.5)
dtm_together_df <- as.data.frame.matrix(dtm_together)
labels <-  c(rep("positive",length(corpus_train_pos)),
rep("negative",length(corpus_train_neg)),
rep("positive",length(corpus_test_pos)),
rep("negative",length(corpus_test_neg)))
dtm_together_df <- cbind(dtm_together_df,labels)
max_col <- ncol(dtm_together_df)
train <- dtm_together_df[1:training_size,1:max_col-1] #last column is the category
dim(train)
dtm_together <- removeSparseTerms(DocumentTermMatrix(fullcorpus), sparse=0.7) #what is this? what happens if I change this??
dtm_together_df <- as.data.frame.matrix(dtm_together)
labels <-  c(rep("positive",length(corpus_train_pos)),
rep("negative",length(corpus_train_neg)),
rep("positive",length(corpus_test_pos)),
rep("negative",length(corpus_test_neg)))
dtm_together_df <- cbind(dtm_together_df,labels)
max_col <- ncol(dtm_together_df)
train <- dtm_together_df[1:training_size,1:max_col-1] #last column is the category
class_train <- dtm_together_df[1:training_size,max_col]
dim(train)
train
library(tm)
library(e1071)
library(SnowballC) #for stemming.
setwd("~/Dropbox/ClassroomSlides-BothCourses/LING410X/Week8Mats/code")
#This function does some basic pre-processing for all files in a folder!
cleanCorpus <- function(folderpath)
{
corpus <- VCorpus(DirSource(folderpath))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus,stemDocument)
return(corpus)
#Note: getTransformations() gives full list of possible transformations.
#stopwords("English") will give you all stopwords.
}
#Create training corpus
corpus_train_pos <- cleanCorpus("largerdataset/train/pos") #has 234 examples of positive reviews
corpus_train_neg <- cleanCorpus("largerdataset/train/neg") #has 234 examples of negative reviews
corpus_train <- c(corpus_train_pos,corpus_train_neg) #has 468 examples in total
training_size <- length(corpus_train_pos) + length(corpus_train_neg)
#Create testing corpus
corpus_test_pos <- cleanCorpus("largerdataset/test/pos") #82 positive docs for testing
corpus_test_neg <- cleanCorpus("largerdataset/test/neg") #67 negative docs for testing
corpus_test <- c(corpus_test_pos,corpus_test_neg) #has 149 examples in total
#Combine them, to create a document term matrix for the whole data first
fullcorpus <- c(corpus_train,corpus_test) #has 617 texts.
#training: 234 positive, 234 negative, followed by testing:82 positive, 67 negative
#Create a document-term matrix (using various settings - any of the below 3 lines work fine.)
dtm_together <- DocumentTermMatrix(fullcorpus)
dim(dtm_together)
findAssocs(dtm_together,"like",0.5)
findAssocs(dtm_together,"bad",0.5)
findAssocs(dtm_together,"bad",0.4)
findAssocs(dtm_together,"bad",0.3)
findAssocs(dtm_together,"bad",0.35)
dim(dtm_together)
findAssocs(dtm_together,"movie",0.35)
findAssocs(dtm_together,"movie",0.5)
findAssocs(dtm_together,"movi",0.5)
library(tm)
library(e1071)
library(SnowballC) #for stemming.
setwd("~/Dropbox/ClassroomSlides-BothCourses/LING410X/Week8Mats/code")
#This function does some basic pre-processing for all files in a folder!
cleanCorpus <- function(folderpath)
{
corpus <- VCorpus(DirSource(folderpath))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeWords, stopwords("english"))
#corpus <- tm_map(corpus,stemDocument)
return(corpus)
#Note: getTransformations() gives full list of possible transformations.
#stopwords("English") will give you all stopwords.
}
#Create training corpus
corpus_train_pos <- cleanCorpus("largerdataset/train/pos") #has 234 examples of positive reviews
corpus_train_neg <- cleanCorpus("largerdataset/train/neg") #has 234 examples of negative reviews
corpus_train <- c(corpus_train_pos,corpus_train_neg) #has 468 examples in total
training_size <- length(corpus_train_pos) + length(corpus_train_neg)
#Create testing corpus
corpus_test_pos <- cleanCorpus("largerdataset/test/pos") #82 positive docs for testing
corpus_test_neg <- cleanCorpus("largerdataset/test/neg") #67 negative docs for testing
corpus_test <- c(corpus_test_pos,corpus_test_neg) #has 149 examples in total
#Combine them, to create a document term matrix for the whole data first
fullcorpus <- c(corpus_train,corpus_test) #has 617 texts.
#training: 234 positive, 234 negative, followed by testing:82 positive, 67 negative
#Create a document-term matrix (using various settings - any of the below 3 lines work fine.)
dtm_together <- DocumentTermMatrix(fullcorpus)
dim(dtm_together)
dim(dtm_together)
findAssocs(dtm_together,"movi",0.5)
findAssocs(dtm_together,"good",0.5)
findAssocs(dtm_together,"good",0.3)
findAssocs(dtm_together,"good",0.2)
findAssocs(dtm_together,"really",0.2)
findAssocs(dtm_together,"horrible",0.2)
findAssocs(dtm_together,"lead",0.2)
findAssocs(dtm_together,"plot",0.2)
findAssocs(dtm_together,"self",0.2)
findAssocs(dtm_together,"friend",0.2)
findAssocs(dtm_together,"like",0.2)
findAssocs(dtm_together,"dislike",0.2)
findAssocs(dtm_together,"dislike",0.4)
findAssocs(dtm_together,"like",0.4)
