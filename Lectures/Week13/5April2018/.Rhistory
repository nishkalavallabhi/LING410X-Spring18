stylo()
library(stylo)
stylo()
install.packages("updateR")
lista <- list(1,2,3,4)
listb <- list(1, "a", "b", TRUE)
vectora <- c(1,2,3,4)
vectora
lista
listb
vectorb <- c(1,"aa2",3,4)
vectorb
?readLines
html <- readLines("http://www.gutenberg.org/files/2446/2446-h/2446-h.htm")
nchar(html)
sum(nchar(html))
text <- readlines("http://www.gutenberg.org/ebooks/2446.txt.utf-8")
text <- readLines("http://www.gutenberg.org/ebooks/2446.txt.utf-8")
nchar(text)
sum(nchar(text))
guardian_key <- "8b40468e-139b-4d90-94ac-1f74d22fd291"
library(GuardianR)
results <- get_guardian("George+Washington", guardian_key)
results <- get_guardian("George+Washington", guardian_key, to.date = "01/01/2011", from.date = "01/01/2015")
results <- get_guardian("George+Washington", apu.key = guardian_key, to.date = "01/01/2011", from.date = "01/01/2015")
results <- get_guardian("George+Washington", api.key = guardian_key, to.date = "01/01/2011", from.date = "01/01/2015")
results <- get_guardian("George+Bush", api.key = guardian_key, to.date = "01/01/2011", from.date = "01/01/2015")
results <- get_guardian("George+Town", api.key = guardian_key, to.date = "01/01/2011", from.date = "01/01/2015")
results <- get_guardian("George", api.key = guardian_key, to.date = "01/01/2011", from.date = "01/01/2015")
results <- get_guardian("George+Town", api.key = guardian_key, to.date = "01-01-2011", from.date = "01-01-2015")
results <- get_guardian("George+Town", api.key = guardian_key, to.date = "2011-01-01", from.date = "2015-01-01")
results <- get_guardian("George+Washington", api.key = guardian_key, to.date = "2011-01-01", from.date = "2015-01-01")
results <- get_guardian("Donald+Trump", api.key = guardian_key, to.date = "2011-01-01", from.date = "2015-01-01")
results <- get_guardian("united+states", section="world", from.date="2014-09-16", to.date="2014-09-16", api.key="8b40468e-139b-4d90-94ac-1f74d22fd291")
results <- get_guardian("Donald+Trump", section=“world”, api.key = guardian_key, to.date = "2011-01-01", from.date = "2015-01-01")
results <- get_guardian("Donald+Trump", section= "world", api.key = guardian_key, to.date = "2011-01-01", from.date = "2015-01-01")
results <- get_guardian("donald+trump", section= "world", api.key = guardian_key, to.date = "2011-01-01", from.date = "2015-01-01")
results <- get_guardian("donald+trump", section= "world", api.key = guardian_key, to.date = "2015-01-01", from.date = "2011-01-01")
results <- get_guardian("donald+trump", section= "world", api.key = guardian_key, to.date = "2015-01-01", from.date = "2015-02-01")
results <- get_guardian("donald+trump", section= "world", api.key = guardian_key, to.date = "2015-02-01", from.date = "2015-01-01")
typeof(results)
results
names(results)
results$sectionName
results$headline
length(results)
results[[1]]$headline
results[[1]]
results[[2]]
results[[3]]
results[[4]]
results["headline"]
results <- get_guardian("donald+trump", section= "world", api.key = guardian_key, to.date = "2015-02-01", from.date = "2015-01-01")
names(results)
data.frame(results[3],results[10])
data.frame(results["sectionName"],results["headline"])
my_df <- data.frame(results["sectionName"],results["headline"])
write.csv(my_df, file = "results.csv")
getwd()
source('~/Dropbox/ClassroomSlides-BothCourses/LING410X/Resources/14Feb2017/KWIC.R', echo=TRUE)
setwd("~/Dropbox/ClassroomSlides-BothCourses/LING410X/Resources/14Feb2017")
source('~/Dropbox/ClassroomSlides-BothCourses/LING410X/Resources/14Feb2017/KWIC.R', echo=TRUE)
source('~/Dropbox/ClassroomSlides-BothCourses/LING410X/Resources/14Feb2017/KWIC.R', echo=TRUE)
setwd("~/Dropbox/ClassroomSlides-BothCourses/LING410X/Resources/14Feb2017")
source('~/Dropbox/ClassroomSlides-BothCourses/LING410X/Resources/14Feb2017/KWIC.R', echo=TRUE)
source('~/Dropbox/ClassroomSlides-BothCourses/LING410X/Resources/14Feb2017/KWIC.R', echo=TRUE)
source('~/Dropbox/ClassroomSlides-BothCourses/LING410X/Resources/14Feb2017/KWIC.R', echo=TRUE)
Gender = c("Female", "Male", "Male", "Female", "Female", "Female")
NewOrFollowUp = c("New", "New", "New", "Follow-up", "New", "Follow-up")
Points = c(1,12,32,4,55,21)
example_xtab <- data.frame(Gender, NewOrFollowUp, Points)
example_xtab
xtabs(Points ~ Gender, data=example_xtab)
xtabs(Points ~ Gender + NewOrFollowUp, data=example_xtab)
library(tm)
?removeSparseTerms
makeFlexTextChunks <- function(doc.object, chunk.size=1000, percentage=TRUE){
paras <- getNodeSet(doc.object,
"/d:TEI/d:text/d:body//d:p",
c(d = "http://www.tei-c.org/ns/1.0"))
words <- paste(sapply(paras,xmlValue), collapse=" ")
words.lower <- tolower(words)
words.lower <- gsub("[^[:alnum:][:space:]']", " ", words.lower)
words.l <- strsplit(words.lower, "\\s+")
word.v <- unlist(words.l)
x <- seq_along(word.v)
if(percentage){
max.length <- length(word.v)/chunk.size
chunks.l <- split(word.v, ceiling(x/max.length))
} else {
chunks.l <- split(word.v, ceiling(x/chunk.size))
#deal with small chunks at the end
if(length(chunks.l[[length(chunks.l)]]) <=
length(chunks.l[[length(chunks.l)]])/2){
chunks.l[[length(chunks.l)-1]] <-
c(chunks.l[[length(chunks.l)-1]],
chunks.l[[length(chunks.l)]])
chunks.l[[length(chunks.l)]] <- NULL
}
}
chunks.l <- lapply(chunks.l, paste, collapse=" ")
chunks.df <- do.call(rbind, chunks.l)
return(chunks.df)
wd <- "~/Dropbox/ClassroomSlides-BothCourses/LING410X/Resources/TextAnalysisWithR-Textbook/Suppl.Materials/"
inputDir <- paste(wd,"data/XMLAuthorCorpus",sep="")
stopwords_path <- paste(wd, "data/stoplist.csv",sep="")
files.v <- dir(path=inputDir , pattern=".*xml")
chunk.size <- 1000
topic.m <- NULL
for(i in 1:length(files.v)){
doc.object <- xmlTreeParse(file.path(inputDir, files.v[i]),
useInternalNodes=TRUE)
chunk.m <- makeFlexTextChunks(doc.object, chunk.size,
percentage=FALSE)
textname <- gsub("\\..*","", files.v[i])
segments.m <- cbind(paste(textname,
segment=1:nrow(chunk.m), sep="_"), chunk.m)
topic.m <- rbind(topic.m, segments.m)
}
documents <- as.data.frame(topic.m, stringsAsFactors=F)
colnames(documents) <- c("id", "text")
library(mallet)
mallet.instances <- mallet.import(documents$id,documents$text,stopwords_path,FALSE,
token.regexp="[\\p{L}']+")
topic.model <- MalletLDA(num.topics=43)
topic.model$loadDocuments(mallet.instances)
#The following line is optional.
#Mallet takes default parameters if you don't specify anything here.
topic.model$setAlphaOptimization(40, 80)
#This starts the training process
topic.model$train(400)
)
makeFlexTextChunks <- function(doc.object, chunk.size=1000, percentage=TRUE){
paras <- getNodeSet(doc.object,
"/d:TEI/d:text/d:body//d:p",
c(d = "http://www.tei-c.org/ns/1.0"))
words <- paste(sapply(paras,xmlValue), collapse=" ")
words.lower <- tolower(words)
words.lower <- gsub("[^[:alnum:][:space:]']", " ", words.lower)
words.l <- strsplit(words.lower, "\\s+")
word.v <- unlist(words.l)
x <- seq_along(word.v)
if(percentage){
max.length <- length(word.v)/chunk.size
chunks.l <- split(word.v, ceiling(x/max.length))
} else {
chunks.l <- split(word.v, ceiling(x/chunk.size))
#deal with small chunks at the end
if(length(chunks.l[[length(chunks.l)]]) <=
length(chunks.l[[length(chunks.l)]])/2){
chunks.l[[length(chunks.l)-1]] <-
c(chunks.l[[length(chunks.l)-1]],
chunks.l[[length(chunks.l)]])
chunks.l[[length(chunks.l)]] <- NULL
}
}
chunks.l <- lapply(chunks.l, paste, collapse=" ")
chunks.df <- do.call(rbind, chunks.l)
return(chunks.df)
}
library(XML)
wd <- "~/Dropbox/ClassroomSlides-BothCourses/LING410X/Resources/TextAnalysisWithR-Textbook/Suppl.Materials/"
inputDir <- paste(wd,"data/XMLAuthorCorpus",sep="")
stopwords_path <- paste(wd, "data/stoplist.csv",sep="")
files.v <- dir(path=inputDir , pattern=".*xml")
chunk.size <- 1000
topic.m <- NULL
for(i in 1:length(files.v)){
doc.object <- xmlTreeParse(file.path(inputDir, files.v[i]),
useInternalNodes=TRUE)
chunk.m <- makeFlexTextChunks(doc.object, chunk.size,
percentage=FALSE)
textname <- gsub("\\..*","", files.v[i])
segments.m <- cbind(paste(textname,
segment=1:nrow(chunk.m), sep="_"), chunk.m)
topic.m <- rbind(topic.m, segments.m)
}
documents <- as.data.frame(topic.m, stringsAsFactors=F)
colnames(documents) <- c("id", "text")
library(mallet)
mallet.instances <- mallet.import(documents$id,documents$text,stopwords_path,FALSE,
token.regexp="[\\p{L}']+")
topic.model <- MalletLDA(num.topics=43)
topic.model$loadDocuments(mallet.instances)
#The following line is optional.
#Mallet takes default parameters if you don't specify anything here.
topic.model$setAlphaOptimization(40, 80)
#This starts the training process
topic.model$train(400)
library(mallet)
mallet.instances <- mallet.import(documents$id,documents$text,stopwords_path,FALSE,
token.regexp="[\\p{L}']+")
topic.model <- MalletLDA(num.topics=10)
topic.model$loadDocuments(mallet.instances)
#The following line is optional.
#Mallet takes default parameters if you don't specify anything here.
topic.model$setAlphaOptimization(40, 80)
#This starts the training process
topic.model$train(400)
?MalletLDA
doc.topics <- mallet.doc.topics(topic.model, smoothed=T, normalized=T)
doc.topics
doc.topics
sum(doc.topics[1,])
sum(doc.topics[2,])
doc.topics <- mallet.doc.topics(topic.model, smoothed=T, normalized=T)
topic.words <- mallet.topic.words(topic.model, smoothed=T, normalized=T)
doc.topics[10,]
doc.topics
topic.words
doc.topics <- mallet.doc.topics(topic.model, smoothed=T, normalized=T)
topic.words <- mallet.topic.words(topic.model, smoothed=T, normalized=T)
topic.words <- mallet.topic.words(topic.model, smoothed=T, normalized=T)
topic.words
mallet.top.words(topic.model, topic.words[7,])
mallet.top.words(topic.model, topic.words[7,], normalized=TRUE)
topic.words[1,]
colnames(topic.words)
dim(topic.words)
setwd("~/Dropbox/ClassroomSlides-BothCourses/LING410X/Resources/4April2017")
english <- scan("DollsHouse-Eng.txt", what = "character", sep = "\n")
english.start <- which(english ==  "DRAMATIS PERSONAE")
english.end <- which(english == "(The sound of a door shutting is heard from below.)")
actual_english <- english[english.start:english.end]
english_lower <- tolower(actual_english_string)
english_words <- strsplit(english_lower, "\\W")
sorted_freqs_english <- sort(table(english_words), decreasing = TRUE)
english <- scan("DollsHouse-Eng.txt", what = "character", sep = "\n")
english.start <- which(english ==  "DRAMATIS PERSONAE")
english.end <- which(english == "(The sound of a door shutting is heard from below.)")
actual_english <- english[english.start:english.end]
actual_english_string <- paste(actual_english, collapse = " ")
english_lower <- tolower(actual_english_string)
english_words <- strsplit(english_lower, "\\W")
sorted_freqs_english <- sort(table(english_words), decreasing = TRUE)
names(sorted_freqs_english)
sorted_freqs_english[1]
sorted_freqs_english[]
ncol(table(english_words))
table(english_words)
?table
ncol(as.data.frame(table(english_words)))
df  = as.data.frame(table(english_words))
colnales(df)
colnames(df)
df["english_words"]
colnames(df)
df["Frew"]
df["Freq"]
colnames(df)
words = df["english_words"]
freqs = df["Freq"]
english <- scan("DollsHouse-Eng.txt", what = "character", sep = "\n")
english.start <- which(english ==  "DRAMATIS PERSONAE")
english.end <- which(english == "(The sound of a door shutting is heard from below.)")
actual_english <- english[english.start:english.end]
actual_english_string <- paste(actual_english, collapse = " ")
english_lower <- tolower(actual_english_string)
english_words <- strsplit(english_lower, "\\W")
mydf <- as.data.frame(table(english_words))
colnames(mydf)
words = mydf["english_words"]
freqs = mydf["Freq"]
library(wordcloud)
wordcloud(words=words,freq = freqs)
length(words)
words
?wordcloud
typeof(words)
words = unlist(words)
freqs = unlist(freqs)
length(words)
length(freqs)
wordcloud(words=words,freq = freqs)
wordcloud(words=words,freq = freqs, min.freq = 5)
wordcloud(words=words,freq = freqs, min.freq = 15)
wordcloud(words=words,freq = freqs, min.freq = 15, max.words = 50)
wordcloud(words=words,freq = freqs, min.freq = 15, max.words = 50, scale=c(4,.5))
wordcloud(words=words,freq = freqs, min.freq = 15, max.words = 50, scale=c(41,.5))
wordcloud(words=words,freq = freqs, min.freq = 15, max.words = 50, scale=c(41,.5), random.order = TRUE)
wordcloud(words=words,freq = freqs, min.freq = 15, max.words = 50, scale=c(41,.5), random.order = FALSE)
wordcloud(words=words,freq = freqs, min.freq = 15, max.words = 10, scale=c(41,.5), random.order = FALSE)
wordcloud(words=words,freq = freqs, min.freq = 15, max.words = 10, scale=c(41,.5), random.order = TRUE)
wordcloud(words=words,freq = freqs, min.freq = 15, max.words = 10, scale=c(41,.5), random.order = FALSE)
par(mfrow(c(1,2)))
par(mfcol(c(1,2)))
par(mfrow=c(1,2))
wordcloud(words=words,freq = freqs, min.freq = 15, max.words = 10, scale=c(41,.5), random.order = TRUE)
wordcloud(words=words,freq = freqs, min.freq = 15, max.words = 10, scale=c(41,.5), random.order = FALSE)
wordcloud(words=words,freq = freqs)
install.packages("stylo")
library(stylo)
setwd("~/Dropbox/ClassroomSlides-BothCourses/LING410X/Resources/4April2017")
stylo()
par(mfrow=c(1,1))
rolling.delta()
setwd("~/Dropbox/ClassroomSlides-BothCourses/LING410X/Resources/4April2017/RollingDelta")
rolling.delta()
rolling.delta()
stylo()
setwd("~/Dropbox/ClassroomSlides-BothCourses/LING410X/Resources/4April2017")
stylo()
stylo()
stylo()
exit()
quit()
